#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜çº§æ•°æ®æ·±åº¦åˆ†æå·¥å…·
æä¾›ä¸“ä¸šçš„ç»Ÿè®¡åˆ†æã€è¶‹åŠ¿åˆ†æã€ç›¸å…³æ€§åˆ†æå’Œå¼‚å¸¸æ£€æµ‹
"""

import os
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# ç»Ÿè®¡åˆ†æåº“
from scipy import stats
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans

# å¯è§†åŒ–åº“
try:
    import plotly.graph_objects as go
    import plotly.express as px
    import plotly.figure_factory as ff
    from plotly.subplots import make_subplots
    import plotly.offline as pyo
    HAS_PLOTLY = True
except ImportError:
    print("âš ï¸ è¯·å®‰è£…plotly: pip install plotly")
    HAS_PLOTLY = False

try:
    import seaborn as sns
    import matplotlib.pyplot as plt
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    HAS_MATPLOTLIB = True
except ImportError:
    HAS_MATPLOTLIB = False

# ====== é…ç½®åŒºåŸŸ ======

EXCEL_FILE = "../../data/æ•°æ®åˆå¹¶ç»“æœ_20250601_1703.xlsx"
OUTPUT_DIR = "../../output/analysis_results"
CHARTS_DIR = os.path.join(OUTPUT_DIR, "å›¾è¡¨")

# ====== é…ç½®åŒºåŸŸç»“æŸ ======


class AdvancedDataAnalyzer:
    """é«˜çº§æ•°æ®æ·±åº¦åˆ†æå™¨"""
    
    def __init__(self, excel_file):
        self.excel_file = excel_file
        self.data_cache = {}
        self.analysis_results = {}
        self.charts = []
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        os.makedirs(CHARTS_DIR, exist_ok=True)
        
    def load_key_datasets(self):
        """åŠ è½½å…³é”®æ•°æ®é›†"""
        print("ğŸ“– åŠ è½½å…³é”®æ•°æ®é›†...")
        
        # å®šä¹‰è¦æ·±åº¦åˆ†æçš„å…³é”®Sheet
        key_sheets = {
            'stock_index': 'æ²ªæ·±300æŒ‡æ•°ï¼ˆ2016-2018ï¼‰',
            'stock_portfolio': 'æ„å»ºæŠ•èµ„ç»„åˆçš„äº”åªè‚¡ç¥¨æ•°æ®ï¼ˆ2016-2018ï¼‰',
            'fund_data': 'å››åªå¼€æ”¾å¼è‚¡ç¥¨å‹åŸºé‡‘çš„å‡€å€¼ï¼ˆ2016-2018å¹´ï¼‰',
            'lpr_rates': 'è´·æ¬¾åŸºç¡€åˆ©ç‡ï¼ˆLPRï¼‰æ•°æ®',
            'shibor_rates': 'Shiboråˆ©ç‡ï¼ˆ2018å¹´ï¼‰',
            'bond_gdp': 'å€ºåˆ¸å­˜é‡è§„æ¨¡ä¸GDPï¼ˆ2010-2020å¹´ï¼‰',
            'stock_indices': 'å›½å†…Aè‚¡ä¸»è¦è‚¡æŒ‡çš„æ—¥æ”¶ç›˜æ•°æ®ï¼ˆ2014-2018ï¼‰'
        }
        
        for key, sheet_name in key_sheets.items():
            try:
                df = pd.read_excel(self.excel_file, sheet_name=sheet_name)
                
                # è¿‡æ»¤å…ƒä¿¡æ¯
                if 'å…ƒä¿¡æ¯' in df.columns or 'æ–‡ä»¶ä¿¡æ¯' in df.columns:
                    meta_start = None
                    for idx, row in df.iterrows():
                        if 'åŸå§‹æ–‡ä»¶å' in str(row.values):
                            meta_start = idx
                            break
                    if meta_start is not None:
                        df = df.iloc[:meta_start]
                
                self.data_cache[key] = df
                print(f"   âœ… {sheet_name}: {len(df)} è¡Œ, {len(df.columns)} åˆ—")
                
            except Exception as e:
                print(f"   âŒ åŠ è½½å¤±è´¥: {sheet_name} - {str(e)}")
        
        print(f"âœ… æˆåŠŸåŠ è½½ {len(self.data_cache)} ä¸ªå…³é”®æ•°æ®é›†")
        return True
    
    def analyze_time_series_trends(self):
        """æ—¶é—´åºåˆ—è¶‹åŠ¿åˆ†æ"""
        print(f"\nğŸ“ˆ æ—¶é—´åºåˆ—è¶‹åŠ¿æ·±åº¦åˆ†æ")
        print("-" * 50)
        
        trend_analysis = {}
        
        # åˆ†ææ²ªæ·±300æŒ‡æ•°è¶‹åŠ¿
        if 'stock_index' in self.data_cache:
            df = self.data_cache['stock_index']
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            
            for col in numeric_cols:
                values = df[col].dropna()
                if len(values) > 30:  # è‡³å°‘30ä¸ªæ•°æ®ç‚¹
                    # è®¡ç®—è¶‹åŠ¿æŒ‡æ ‡
                    x = np.arange(len(values))
                    slope, intercept, r_value, p_value, std_err = stats.linregress(x, values)
                    
                    # è®¡ç®—ç§»åŠ¨å¹³å‡
                    ma_20 = values.rolling(window=20).mean()
                    ma_50 = values.rolling(window=50).mean() if len(values) > 50 else None
                    
                    # è®¡ç®—æ³¢åŠ¨ç‡
                    returns = values.pct_change().dropna()
                    volatility = returns.std() * np.sqrt(252)  # å¹´åŒ–æ³¢åŠ¨ç‡
                    
                    trend_analysis[f'æ²ªæ·±300_{col}'] = {
                        'slope': slope,
                        'r_squared': r_value**2,
                        'p_value': p_value,
                        'volatility': volatility,
                        'trend_strength': abs(slope) * r_value**2,
                        'direction': 'upward' if slope > 0 else 'downward'
                    }
        
        # åˆ†æåˆ©ç‡è¶‹åŠ¿
        if 'lpr_rates' in self.data_cache:
            df = self.data_cache['lpr_rates']
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            
            for col in numeric_cols:
                values = df[col].dropna()
                if len(values) > 10:
                    # åˆ©ç‡å˜åŒ–åˆ†æ
                    rate_changes = values.diff().dropna()
                    
                    trend_analysis[f'LPR_{col}'] = {
                        'mean_rate': values.mean(),
                        'rate_range': values.max() - values.min(),
                        'volatility': values.std(),
                        'recent_trend': values.tail(10).mean() - values.head(10).mean(),
                        'change_frequency': (rate_changes != 0).sum()
                    }
        
        self.analysis_results['trends'] = trend_analysis
        
        # æ‰“å°å…³é”®å‘ç°
        print("å…³é”®è¶‹åŠ¿å‘ç°:")
        for metric, data in trend_analysis.items():
            if 'slope' in data:
                trend = "ä¸Šå‡" if data['direction'] == 'upward' else "ä¸‹é™"
                strength = "å¼º" if data['trend_strength'] > 0.5 else "å¼±"
                print(f"   ğŸ“Š {metric}: {trend}è¶‹åŠ¿ï¼Œå¼ºåº¦{strength} (RÂ²={data['r_squared']:.3f})")
        
        return trend_analysis
    
    def analyze_correlations(self):
        """ç›¸å…³æ€§æ·±åº¦åˆ†æ"""
        print(f"\nğŸ”— ç›¸å…³æ€§æ·±åº¦åˆ†æ")
        print("-" * 50)
        
        correlation_results = {}
        
        # è‚¡ç¥¨æŠ•èµ„ç»„åˆç›¸å…³æ€§åˆ†æ
        if 'stock_portfolio' in self.data_cache:
            df = self.data_cache['stock_portfolio']
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            
            if len(numeric_cols) >= 2:
                # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
                corr_matrix = df[numeric_cols].corr()
                
                # æ‰¾å‡ºé«˜ç›¸å…³æ€§å¯¹
                high_corr_pairs = []
                for i in range(len(corr_matrix.columns)):
                    for j in range(i+1, len(corr_matrix.columns)):
                        corr_val = corr_matrix.iloc[i, j]
                        if abs(corr_val) > 0.7:
                            high_corr_pairs.append({
                                'pair': f"{corr_matrix.columns[i]} - {corr_matrix.columns[j]}",
                                'correlation': corr_val,
                                'strength': 'strong' if abs(corr_val) > 0.8 else 'moderate'
                            })
                
                correlation_results['portfolio_correlations'] = {
                    'matrix': corr_matrix,
                    'high_correlations': high_corr_pairs
                }
        
        # åŸºé‡‘é—´ç›¸å…³æ€§åˆ†æ
        if 'fund_data' in self.data_cache:
            df = self.data_cache['fund_data']
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            
            if len(numeric_cols) >= 2:
                # è®¡ç®—æ”¶ç›Šç‡ç›¸å…³æ€§
                returns_df = df[numeric_cols].pct_change().dropna()
                returns_corr = returns_df.corr()
                
                correlation_results['fund_correlations'] = returns_corr
        
        self.analysis_results['correlations'] = correlation_results
        
        # æ‰“å°ç›¸å…³æ€§å‘ç°
        if 'portfolio_correlations' in correlation_results:
            high_corr = correlation_results['portfolio_correlations']['high_correlations']
            print(f"å‘ç° {len(high_corr)} ä¸ªé«˜ç›¸å…³æ€§è‚¡ç¥¨å¯¹:")
            for pair_info in high_corr[:5]:
                print(f"   ğŸ”— {pair_info['pair']}: {pair_info['correlation']:.3f} ({pair_info['strength']})")
        
        return correlation_results
    
    def analyze_risk_metrics(self):
        """é£é™©æŒ‡æ ‡åˆ†æ"""
        print(f"\nâš ï¸ é£é™©æŒ‡æ ‡æ·±åº¦åˆ†æ")
        print("-" * 50)
        
        risk_analysis = {}
        
        # è‚¡ç¥¨é£é™©åˆ†æ
        if 'stock_portfolio' in self.data_cache:
            df = self.data_cache['stock_portfolio']
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            
            stock_risks = {}
            for col in numeric_cols:
                values = df[col].dropna()
                if len(values) > 30:
                    returns = values.pct_change().dropna()
                    
                    # è®¡ç®—é£é™©æŒ‡æ ‡
                    stock_risks[col] = {
                        'volatility': returns.std() * np.sqrt(252),  # å¹´åŒ–æ³¢åŠ¨ç‡
                        'var_95': np.percentile(returns, 5),  # 95% VaR
                        'var_99': np.percentile(returns, 1),  # 99% VaR
                        'max_drawdown': self._calculate_max_drawdown(values),
                        'sharpe_ratio': self._calculate_sharpe_ratio(returns),
                        'skewness': returns.skew(),
                        'kurtosis': returns.kurtosis()
                    }
            
            risk_analysis['stock_risks'] = stock_risks
        
        # åŸºé‡‘é£é™©åˆ†æ
        if 'fund_data' in self.data_cache:
            df = self.data_cache['fund_data']
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            
            fund_risks = {}
            for col in numeric_cols:
                values = df[col].dropna()
                if len(values) > 30:
                    returns = values.pct_change().dropna()
                    
                    # æ‰¾ä¸€ä¸ªåŸºå‡†åˆ—ï¼ˆç¬¬ä¸€ä¸ªæ•°å€¼åˆ—ï¼‰
                    benchmark_col = None
                    for benchmark_candidate in numeric_cols:
                        if benchmark_candidate != col:
                            benchmark_values = df[benchmark_candidate].dropna()
                            if len(benchmark_values) > 30:
                                benchmark_col = benchmark_candidate
                                break
                    
                    beta_value = None
                    if benchmark_col:
                        benchmark_returns = df[benchmark_col].pct_change().dropna()
                        # ç¡®ä¿é•¿åº¦åŒ¹é…
                        min_length = min(len(returns), len(benchmark_returns))
                        if min_length > 10:
                            beta_value = self._calculate_beta(
                                returns.iloc[:min_length], 
                                benchmark_returns.iloc[:min_length]
                            )
                    
                    fund_risks[col] = {
                        'volatility': returns.std() * np.sqrt(252),
                        'tracking_error': returns.std() * np.sqrt(252),
                        'max_drawdown': self._calculate_max_drawdown(values),
                        'calmar_ratio': self._calculate_calmar_ratio(values, returns),
                        'beta': beta_value
                    }
            
            risk_analysis['fund_risks'] = fund_risks
        
        self.analysis_results['risks'] = risk_analysis
        
        # æ‰“å°é£é™©å‘ç°
        print("å…³é”®é£é™©æŒ‡æ ‡:")
        if 'stock_risks' in risk_analysis:
            for stock, metrics in list(risk_analysis['stock_risks'].items())[:3]:
                vol = metrics['volatility'] * 100
                sr = metrics['sharpe_ratio']
                print(f"   ğŸ“Š {stock}: æ³¢åŠ¨ç‡ {vol:.1f}%, å¤æ™®æ¯”ç‡ {sr:.2f}")
        
        return risk_analysis
    
    def detect_anomalies(self):
        """å¼‚å¸¸å€¼æ£€æµ‹"""
        print(f"\nğŸ” å¼‚å¸¸å€¼æ£€æµ‹åˆ†æ")
        print("-" * 50)
        
        anomaly_results = {}
        
        # å¯¹ä¸»è¦æ•°æ®é›†è¿›è¡Œå¼‚å¸¸æ£€æµ‹
        for dataset_name, df in self.data_cache.items():
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            dataset_anomalies = {}
            
            for col in numeric_cols:
                values = df[col].dropna()
                if len(values) > 10:
                    # Z-scoreå¼‚å¸¸æ£€æµ‹
                    z_scores = np.abs(stats.zscore(values))
                    z_anomalies = (z_scores > 3).sum()
                    
                    # IQRå¼‚å¸¸æ£€æµ‹
                    Q1 = values.quantile(0.25)
                    Q3 = values.quantile(0.75)
                    IQR = Q3 - Q1
                    lower_bound = Q1 - 1.5 * IQR
                    upper_bound = Q3 + 1.5 * IQR
                    iqr_anomalies = ((values < lower_bound) | (values > upper_bound)).sum()
                    
                    dataset_anomalies[col] = {
                        'z_score_anomalies': z_anomalies,
                        'iqr_anomalies': iqr_anomalies,
                        'anomaly_percentage': (iqr_anomalies / len(values)) * 100
                    }
            
            anomaly_results[dataset_name] = dataset_anomalies
        
        self.analysis_results['anomalies'] = anomaly_results
        
        # æ‰“å°å¼‚å¸¸æ£€æµ‹ç»“æœ
        print("å¼‚å¸¸å€¼æ£€æµ‹ç»“æœ:")
        for dataset, anomalies in anomaly_results.items():
            total_anomalies = sum(info['iqr_anomalies'] for info in anomalies.values())
            if total_anomalies > 0:
                print(f"   âš ï¸ {dataset}: å‘ç° {total_anomalies} ä¸ªå¼‚å¸¸å€¼")
        
        return anomaly_results
    
    def perform_clustering_analysis(self):
        """èšç±»åˆ†æ"""
        print(f"\nğŸ¯ èšç±»åˆ†æ")
        print("-" * 50)
        
        clustering_results = {}
        
        # å¯¹è‚¡ç¥¨æŠ•èµ„ç»„åˆè¿›è¡Œèšç±»
        if 'stock_portfolio' in self.data_cache:
            df = self.data_cache['stock_portfolio']
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            
            if len(numeric_cols) >= 2:
                # å‡†å¤‡æ•°æ®
                data = df[numeric_cols].dropna()
                
                if len(data) > 10:
                    # æ ‡å‡†åŒ–
                    scaler = StandardScaler()
                    scaled_data = scaler.fit_transform(data)
                    
                    # K-meansèšç±»
                    kmeans = KMeans(n_clusters=3, random_state=42)
                    clusters = kmeans.fit_predict(scaled_data)
                    
                    # åˆ†æèšç±»ç»“æœ
                    cluster_analysis = {}
                    for i in range(3):
                        cluster_mask = clusters == i
                        cluster_data = data[cluster_mask]
                        cluster_analysis[f'cluster_{i}'] = {
                            'size': cluster_mask.sum(),
                            'mean_values': cluster_data.mean().to_dict(),
                            'characteristics': self._analyze_cluster_characteristics(cluster_data)
                        }
                    
                    clustering_results['stock_clusters'] = cluster_analysis
        
        self.analysis_results['clustering'] = clustering_results
        
        # æ‰“å°èšç±»ç»“æœ
        if 'stock_clusters' in clustering_results:
            print("è‚¡ç¥¨èšç±»åˆ†æç»“æœ:")
            for cluster_id, info in clustering_results['stock_clusters'].items():
                print(f"   ğŸ“Š {cluster_id}: {info['size']} ä¸ªæ•°æ®ç‚¹")
        
        return clustering_results
    
    def _calculate_max_drawdown(self, values):
        """è®¡ç®—æœ€å¤§å›æ’¤"""
        peak = values.expanding().max()
        drawdown = (values - peak) / peak
        return drawdown.min()
    
    def _calculate_sharpe_ratio(self, returns, risk_free_rate=0.02):
        """è®¡ç®—å¤æ™®æ¯”ç‡"""
        excess_returns = returns - risk_free_rate / 252
        if excess_returns.std() == 0:
            return 0
        return excess_returns.mean() / excess_returns.std() * np.sqrt(252)
    
    def _calculate_calmar_ratio(self, values, returns):
        """è®¡ç®—å¡ç›æ¯”ç‡"""
        annual_return = (values.iloc[-1] / values.iloc[0]) ** (252 / len(values)) - 1
        max_dd = abs(self._calculate_max_drawdown(values))
        return annual_return / max_dd if max_dd != 0 else 0
    
    def _calculate_beta(self, stock_returns, market_returns):
        """è®¡ç®—Betaç³»æ•°"""
        if len(stock_returns) == len(market_returns):
            covariance = np.cov(stock_returns, market_returns)[0][1]
            market_variance = np.var(market_returns)
            return covariance / market_variance if market_variance != 0 else 0
        return None
    
    def _analyze_cluster_characteristics(self, cluster_data):
        """åˆ†æèšç±»ç‰¹å¾"""
        characteristics = {}
        for col in cluster_data.columns:
            values = cluster_data[col]
            characteristics[col] = {
                'mean': values.mean(),
                'std': values.std(),
                'volatility_level': 'high' if values.std() > values.mean() * 0.1 else 'low'
            }
        return characteristics
    
    def generate_insights_summary(self):
        """ç”Ÿæˆæ·±åº¦æ´å¯Ÿæ€»ç»“"""
        print(f"\nğŸ¯ ç”Ÿæˆæ·±åº¦åˆ†ææ´å¯Ÿ")
        print("-" * 50)
        
        insights = []
        
        # è¶‹åŠ¿æ´å¯Ÿ
        if 'trends' in self.analysis_results:
            trends = self.analysis_results['trends']
            strong_trends = [k for k, v in trends.items() if v.get('trend_strength', 0) > 0.5]
            if strong_trends:
                insights.append(f"å‘ç° {len(strong_trends)} ä¸ªå¼ºè¶‹åŠ¿æŒ‡æ ‡")
        
        # ç›¸å…³æ€§æ´å¯Ÿ
        if 'correlations' in self.analysis_results:
            corr = self.analysis_results['correlations']
            if 'portfolio_correlations' in corr:
                high_corr_count = len(corr['portfolio_correlations']['high_correlations'])
                if high_corr_count > 0:
                    insights.append(f"æŠ•èµ„ç»„åˆä¸­å‘ç° {high_corr_count} å¯¹é«˜ç›¸å…³æ€§èµ„äº§")
        
        # é£é™©æ´å¯Ÿ
        if 'risks' in self.analysis_results:
            risks = self.analysis_results['risks']
            if 'stock_risks' in risks:
                high_risk_assets = [k for k, v in risks['stock_risks'].items() if v.get('volatility', 0) > 0.3]
                insights.append(f"è¯†åˆ«å‡º {len(high_risk_assets)} ä¸ªé«˜é£é™©èµ„äº§")
        
        # å¼‚å¸¸å€¼æ´å¯Ÿ
        if 'anomalies' in self.analysis_results:
            anomalies = self.analysis_results['anomalies']
            total_anomalies = sum(
                sum(col_info['iqr_anomalies'] for col_info in dataset.values())
                for dataset in anomalies.values()
            )
            if total_anomalies > 0:
                insights.append(f"æ£€æµ‹åˆ° {total_anomalies} ä¸ªæ½œåœ¨å¼‚å¸¸å€¼")
        
        print("å…³é”®æ´å¯Ÿ:")
        for insight in insights:
            print(f"   ğŸ’¡ {insight}")
        
        # ä¿å­˜æ´å¯ŸæŠ¥å‘Š
        insights_file = os.path.join(OUTPUT_DIR, f"æ·±åº¦åˆ†ææ´å¯Ÿ_{datetime.now().strftime('%Y%m%d_%H%M')}.txt")
        with open(insights_file, 'w', encoding='utf-8') as f:
            f.write("æ·±åº¦æ•°æ®åˆ†ææ´å¯ŸæŠ¥å‘Š\n")
            f.write("=" * 50 + "\n")
            f.write(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            for category, results in self.analysis_results.items():
                f.write(f"{category.upper()} åˆ†æç»“æœ:\n")
                f.write(f"{str(results)}\n\n")
        
        print(f"âœ… æ·±åº¦åˆ†ææ´å¯Ÿå·²ä¿å­˜åˆ°: {insights_file}")
        return insights
    
    def run_advanced_analysis(self):
        """è¿è¡Œé«˜çº§åˆ†æ"""
        print("ğŸš€ å¼€å§‹é«˜çº§æ•°æ®æ·±åº¦åˆ†æ")
        print("=" * 60)
        
        # 1. åŠ è½½æ•°æ®
        if not self.load_key_datasets():
            return False
        
        # 2. æ—¶é—´åºåˆ—è¶‹åŠ¿åˆ†æ
        self.analyze_time_series_trends()
        
        # 3. ç›¸å…³æ€§åˆ†æ
        self.analyze_correlations()
        
        # 4. é£é™©æŒ‡æ ‡åˆ†æ
        self.analyze_risk_metrics()
        
        # 5. å¼‚å¸¸å€¼æ£€æµ‹
        self.detect_anomalies()
        
        # 6. èšç±»åˆ†æ
        self.perform_clustering_analysis()
        
        # 7. ç”Ÿæˆæ´å¯Ÿæ€»ç»“
        insights = self.generate_insights_summary()
        
        print(f"\nğŸ‰ é«˜çº§åˆ†æå®Œæˆ!")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {OUTPUT_DIR}")
        
        return True


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ“Š é«˜çº§æ•°æ®æ·±åº¦åˆ†æå·¥å…·")
    print("=" * 40)
    
    if not os.path.exists(EXCEL_FILE):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {EXCEL_FILE}")
        return
    
    analyzer = AdvancedDataAnalyzer(EXCEL_FILE)
    analyzer.run_advanced_analysis()


if __name__ == "__main__":
    main() 
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šSheetæ•°æ®åˆ†æå·¥å…·
ä¸“é—¨åˆ†æåˆå¹¶åçš„Excelæ–‡ä»¶ï¼Œæä¾›å…¨é¢çš„æ•°æ®æ´å¯Ÿå’Œåˆ†ææŠ¥å‘Š
"""

import os
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# å°è¯•å¯¼å…¥å¯è§†åŒ–åº“
try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    HAS_PLOTTING = True
except ImportError:
    print("ğŸ“Š æç¤º: å®‰è£… matplotlib å’Œ seaborn å¯è·å¾—å¯è§†åŒ–åŠŸèƒ½")
    HAS_PLOTTING = False

# ====== é…ç½®åŒºåŸŸ ======

# è¾“å…¥æ–‡ä»¶é…ç½®
EXCEL_FILES = [
    "æ•°æ®åˆå¹¶ç»“æœ_20250601_1703.xlsx",  # æœ€æ–°åˆå¹¶æ–‡ä»¶
    "å®Œæ•´é‡‘èæ•°æ®åˆå¹¶_20250601_1658.xlsx",  # å®Œæ•´é‡‘èæ•°æ®
    "å¤šè¡¨åˆå¹¶æ•°æ®_20250601_1658.xlsx",  # å¤šè¡¨åˆå¹¶æ•°æ®
]

# è¾“å‡ºé…ç½®
ANALYSIS_OUTPUT_DIR = "åˆ†æç»“æœ"
GENERATE_CHARTS = True
SAVE_DETAILED_REPORTS = True

# ====== é…ç½®åŒºåŸŸç»“æŸ ======


class MultiSheetAnalyzer:
    """å¤šSheetæ•°æ®åˆ†æå™¨"""
    
    def __init__(self, excel_file):
        self.excel_file = excel_file
        self.sheets_data = {}
        self.summary_info = {}
        self.analysis_results = {}
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(ANALYSIS_OUTPUT_DIR, exist_ok=True)
        
    def load_excel_data(self):
        """åŠ è½½Excelæ–‡ä»¶çš„æ‰€æœ‰Sheetæ•°æ®"""
        print(f"ğŸ“– æ­£åœ¨åŠ è½½Excelæ–‡ä»¶: {self.excel_file}")
        
        try:
            # è·å–æ‰€æœ‰Sheetåç§°
            excel_file = pd.ExcelFile(self.excel_file)
            sheet_names = excel_file.sheet_names
            
            print(f"   ğŸ“Š å‘ç° {len(sheet_names)} ä¸ªSheet")
            
            # è¯»å–æ¯ä¸ªSheet
            for i, sheet_name in enumerate(sheet_names, 1):
                try:
                    print(f"   [{i}/{len(sheet_names)}] è¯»å–Sheet: {sheet_name}")
                    
                    df = pd.read_excel(self.excel_file, sheet_name=sheet_name)
                    
                    # è¿‡æ»¤æ‰å…ƒä¿¡æ¯è¡Œï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    if 'å…ƒä¿¡æ¯' in df.columns or 'æ–‡ä»¶ä¿¡æ¯' in df.columns:
                        # æ‰¾åˆ°å…ƒä¿¡æ¯å¼€å§‹çš„ä½ç½®
                        meta_start = None
                        for idx, row in df.iterrows():
                            if 'åŸå§‹æ–‡ä»¶å' in str(row.values):
                                meta_start = idx
                                break
                        
                        if meta_start is not None:
                            df = df.iloc[:meta_start]  # åªä¿ç•™æ•°æ®éƒ¨åˆ†
                    
                    self.sheets_data[sheet_name] = df
                    print(f"      âœ… æˆåŠŸè¯»å–: {len(df)} è¡Œ, {len(df.columns)} åˆ—")
                    
                except Exception as e:
                    print(f"      âŒ è¯»å–å¤±è´¥: {str(e)}")
                    continue
            
            print(f"âœ… æˆåŠŸåŠ è½½ {len(self.sheets_data)} ä¸ªSheetçš„æ•°æ®")
            return True
            
        except Exception as e:
            print(f"âŒ åŠ è½½Excelæ–‡ä»¶å¤±è´¥: {str(e)}")
            return False
    
    def analyze_data_overview(self):
        """æ•°æ®æ¦‚è§ˆåˆ†æ"""
        print(f"\nğŸ“Š æ•°æ®æ¦‚è§ˆåˆ†æ")
        print("-" * 50)
        
        total_rows = 0
        total_cols = 0
        sheet_info = []
        
        for sheet_name, df in self.sheets_data.items():
            if sheet_name == 'ğŸ“Šæ±‡æ€»ä¿¡æ¯':  # è·³è¿‡æ±‡æ€»Sheet
                continue
                
            rows = len(df)
            cols = len(df.columns)
            total_rows += rows
            
            sheet_info.append({
                'sheet_name': sheet_name,
                'rows': rows,
                'columns': cols,
                'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024 / 1024,
                'non_null_ratio': df.count().sum() / (rows * cols) if rows * cols > 0 else 0
            })
        
        # æ’åºï¼šæŒ‰è¡Œæ•°æ’åº
        sheet_info.sort(key=lambda x: x['rows'], reverse=True)
        
        print(f"æ•°æ®é›†æ€»è§ˆ:")
        print(f"   ğŸ“„ æ€»Sheetæ•°: {len(sheet_info)}")
        print(f"   ğŸ“ˆ æ€»æ•°æ®è¡Œæ•°: {total_rows:,}")
        print(f"   ğŸ’¾ æ€»å†…å­˜å ç”¨: {sum(info['memory_usage_mb'] for info in sheet_info):.1f} MB")
        
        print(f"\nğŸ“‹ å„Sheetè¯¦ç»†ä¿¡æ¯:")
        print(f"{'Sheetåç§°':<30} {'è¡Œæ•°':<10} {'åˆ—æ•°':<8} {'æ•°æ®å®Œæ•´åº¦':<10}")
        print("-" * 65)
        
        for info in sheet_info[:10]:  # æ˜¾ç¤ºå‰10ä¸ªæœ€å¤§çš„Sheet
            completeness = f"{info['non_null_ratio']*100:.1f}%"
            print(f"{info['sheet_name']:<30} {info['rows']:<10,} {info['columns']:<8} {completeness:<10}")
        
        if len(sheet_info) > 10:
            print(f"... è¿˜æœ‰ {len(sheet_info) - 10} ä¸ªSheet")
        
        self.analysis_results['overview'] = {
            'total_sheets': len(sheet_info),
            'total_rows': total_rows,
            'sheet_info': sheet_info
        }
        
        return sheet_info
    
    def analyze_data_types(self):
        """æ•°æ®ç±»å‹åˆ†æ"""
        print(f"\nğŸ” æ•°æ®ç±»å‹åˆ†æ")
        print("-" * 50)
        
        all_numeric_cols = set()
        all_text_cols = set()
        all_date_cols = set()
        common_columns = None
        
        for sheet_name, df in self.sheets_data.items():
            if sheet_name == 'ğŸ“Šæ±‡æ€»ä¿¡æ¯':
                continue
            
            # åˆ†æåˆ—ç±»å‹
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            text_cols = df.select_dtypes(include=['object']).columns.tolist()
            date_cols = df.select_dtypes(include=['datetime']).columns.tolist()
            
            # å°è¯•è¯†åˆ«å¯èƒ½çš„æ—¥æœŸåˆ—
            potential_date_cols = []
            for col in text_cols:
                if any(keyword in col.lower() for keyword in ['date', 'æ—¥æœŸ', 'time', 'æ—¶é—´', 'å¹´', 'æœˆ', 'æ—¥']):
                    potential_date_cols.append(col)
            
            all_numeric_cols.update(numeric_cols)
            all_text_cols.update(text_cols)
            all_date_cols.update(date_cols + potential_date_cols)
            
            # æ‰¾å‡ºå…±åŒåˆ—
            if common_columns is None:
                common_columns = set(df.columns)
            else:
                common_columns = common_columns.intersection(set(df.columns))
        
        print(f"æ•°æ®ç±»å‹ç»Ÿè®¡:")
        print(f"   ğŸ”¢ æ•°å€¼å‹åˆ—: {len(all_numeric_cols)} ä¸ª")
        print(f"   ğŸ“ æ–‡æœ¬å‹åˆ—: {len(all_text_cols)} ä¸ª")
        print(f"   ğŸ“… æ—¥æœŸå‹åˆ—: {len(all_date_cols)} ä¸ª")
        print(f"   ğŸ”— å…±åŒåˆ—: {len(common_columns)} ä¸ª")
        
        if len(common_columns) > 0:
            print(f"\nå…±åŒåˆ—å:")
            for col in sorted(list(common_columns))[:10]:
                print(f"   - {col}")
            if len(common_columns) > 10:
                print(f"   ... è¿˜æœ‰ {len(common_columns) - 10} ä¸ª")
        
        self.analysis_results['data_types'] = {
            'numeric_columns': list(all_numeric_cols),
            'text_columns': list(all_text_cols),
            'date_columns': list(all_date_cols),
            'common_columns': list(common_columns)
        }
        
        return all_numeric_cols, all_text_cols, all_date_cols, common_columns
    
    def analyze_data_quality(self):
        """æ•°æ®è´¨é‡åˆ†æ"""
        print(f"\nğŸ¯ æ•°æ®è´¨é‡åˆ†æ")
        print("-" * 50)
        
        quality_report = []
        
        for sheet_name, df in self.sheets_data.items():
            if sheet_name == 'ğŸ“Šæ±‡æ€»ä¿¡æ¯':
                continue
            
            # è®¡ç®—å„ç§è´¨é‡æŒ‡æ ‡
            total_cells = len(df) * len(df.columns)
            null_cells = df.isnull().sum().sum()
            duplicate_rows = df.duplicated().sum()
            
            # æ•°å€¼åˆ—çš„ç»Ÿè®¡
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            numeric_stats = {}
            
            if len(numeric_cols) > 0:
                for col in numeric_cols:
                    stats = {
                        'mean': df[col].mean(),
                        'std': df[col].std(),
                        'min': df[col].min(),
                        'max': df[col].max(),
                        'zeros': (df[col] == 0).sum(),
                        'negatives': (df[col] < 0).sum()
                    }
                    numeric_stats[col] = stats
            
            quality_info = {
                'sheet_name': sheet_name,
                'total_cells': total_cells,
                'null_cells': null_cells,
                'null_percentage': (null_cells / total_cells * 100) if total_cells > 0 else 0,
                'duplicate_rows': duplicate_rows,
                'duplicate_percentage': (duplicate_rows / len(df) * 100) if len(df) > 0 else 0,
                'numeric_stats': numeric_stats
            }
            
            quality_report.append(quality_info)
        
        # æ˜¾ç¤ºè´¨é‡æŠ¥å‘Š
        print(f"æ•°æ®è´¨é‡æŠ¥å‘Š:")
        print(f"{'Sheetåç§°':<30} {'ç¼ºå¤±ç‡':<10} {'é‡å¤ç‡':<10} {'æ•°å€¼åˆ—æ•°':<10}")
        print("-" * 70)
        
        for info in quality_report[:10]:
            null_pct = f"{info['null_percentage']:.1f}%"
            dup_pct = f"{info['duplicate_percentage']:.1f}%"
            num_cols = len(info['numeric_stats'])
            print(f"{info['sheet_name']:<30} {null_pct:<10} {dup_pct:<10} {num_cols:<10}")
        
        self.analysis_results['quality'] = quality_report
        return quality_report
    
    def analyze_numerical_data(self):
        """æ•°å€¼æ•°æ®åˆ†æ"""
        print(f"\nğŸ“ˆ æ•°å€¼æ•°æ®åˆ†æ")
        print("-" * 50)
        
        all_numeric_data = []
        numeric_summary = {}
        
        for sheet_name, df in self.sheets_data.items():
            if sheet_name == 'ğŸ“Šæ±‡æ€»ä¿¡æ¯':
                continue
            
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            
            if len(numeric_cols) > 0:
                for col in numeric_cols:
                    values = df[col].dropna()
                    if len(values) > 0:
                        all_numeric_data.extend(values)
                        
                        if col not in numeric_summary:
                            numeric_summary[col] = {
                                'total_values': 0,
                                'sum': 0,
                                'min': float('inf'),
                                'max': float('-inf'),
                                'sheets': []
                            }
                        
                        numeric_summary[col]['total_values'] += len(values)
                        numeric_summary[col]['sum'] += values.sum()
                        numeric_summary[col]['min'] = min(numeric_summary[col]['min'], values.min())
                        numeric_summary[col]['max'] = max(numeric_summary[col]['max'], values.max())
                        numeric_summary[col]['sheets'].append(sheet_name)
        
        # æ˜¾ç¤ºæ•°å€¼åˆ†æç»“æœ
        if all_numeric_data:
            print(f"æ•°å€¼æ•°æ®æ¦‚è¿°:")
            print(f"   ğŸ“Š æ€»æ•°å€¼ä¸ªæ•°: {len(all_numeric_data):,}")
            print(f"   ğŸ“ˆ æ•°å€¼èŒƒå›´: {min(all_numeric_data):.2f} ~ {max(all_numeric_data):.2f}")
            print(f"   ğŸ“Š å¹³å‡å€¼: {np.mean(all_numeric_data):.2f}")
            print(f"   ğŸ“Š ä¸­ä½æ•°: {np.median(all_numeric_data):.2f}")
            
            print(f"\nä¸»è¦æ•°å€¼åˆ—åˆ†æ:")
            print(f"{'åˆ—å':<25} {'æ€»å€¼æ•°':<10} {'æœ€å°å€¼':<12} {'æœ€å¤§å€¼':<12} {'å¹³å‡å€¼':<12}")
            print("-" * 80)
            
            # æŒ‰æ€»å€¼æ•°æ’åºï¼Œæ˜¾ç¤ºå‰10ä¸ª
            sorted_cols = sorted(numeric_summary.items(), 
                               key=lambda x: x[1]['total_values'], reverse=True)
            
            for col_name, stats in sorted_cols[:10]:
                avg_val = stats['sum'] / stats['total_values']
                print(f"{col_name:<25} {stats['total_values']:<10,} {stats['min']:<12.2f} {stats['max']:<12.2f} {avg_val:<12.2f}")
        
        self.analysis_results['numerical'] = {
            'total_values': len(all_numeric_data),
            'column_summary': numeric_summary
        }
        
        return numeric_summary
    
    def find_patterns_and_insights(self):
        """å¯»æ‰¾æ•°æ®æ¨¡å¼å’Œæ´å¯Ÿ"""
        print(f"\nğŸ” æ•°æ®æ¨¡å¼å’Œæ´å¯Ÿåˆ†æ")
        print("-" * 50)
        
        insights = []
        
        # 1. å¯»æ‰¾é«˜ç›¸å…³æ€§çš„æ•°å€¼åˆ—
        correlations = []
        for sheet_name, df in self.sheets_data.items():
            if sheet_name == 'ğŸ“Šæ±‡æ€»ä¿¡æ¯':
                continue
                
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) >= 2:
                try:
                    corr_matrix = df[numeric_cols].corr()
                    # æ‰¾å‡ºé«˜ç›¸å…³æ€§çš„åˆ—å¯¹
                    for i in range(len(numeric_cols)):
                        for j in range(i+1, len(numeric_cols)):
                            corr_val = corr_matrix.iloc[i, j]
                            if abs(corr_val) > 0.7 and not np.isnan(corr_val):
                                correlations.append({
                                    'sheet': sheet_name,
                                    'col1': numeric_cols[i],
                                    'col2': numeric_cols[j],
                                    'correlation': corr_val
                                })
                except:
                    continue
        
        if correlations:
            print(f"å‘ç°é«˜ç›¸å…³æ€§åˆ—å¯¹:")
            correlations.sort(key=lambda x: abs(x['correlation']), reverse=True)
            for corr in correlations[:5]:
                print(f"   ğŸ“Š {corr['sheet']}: {corr['col1']} â†” {corr['col2']} (ç›¸å…³æ€§: {corr['correlation']:.3f})")
        
        # 2. å¯»æ‰¾å¼‚å¸¸å€¼
        outliers = []
        for sheet_name, df in self.sheets_data.items():
            if sheet_name == 'ğŸ“Šæ±‡æ€»ä¿¡æ¯':
                continue
                
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            for col in numeric_cols:
                values = df[col].dropna()
                if len(values) > 10:  # è¶³å¤Ÿçš„æ•°æ®ç‚¹
                    Q1 = values.quantile(0.25)
                    Q3 = values.quantile(0.75)
                    IQR = Q3 - Q1
                    lower_bound = Q1 - 1.5 * IQR
                    upper_bound = Q3 + 1.5 * IQR
                    
                    outlier_count = ((values < lower_bound) | (values > upper_bound)).sum()
                    if outlier_count > 0:
                        outliers.append({
                            'sheet': sheet_name,
                            'column': col,
                            'outlier_count': outlier_count,
                            'outlier_percentage': outlier_count / len(values) * 100
                        })
        
        if outliers:
            print(f"\nå‘ç°æ•°æ®å¼‚å¸¸å€¼:")
            outliers.sort(key=lambda x: x['outlier_percentage'], reverse=True)
            for outlier in outliers[:5]:
                print(f"   âš ï¸  {outlier['sheet']}.{outlier['column']}: {outlier['outlier_count']} ä¸ªå¼‚å¸¸å€¼ ({outlier['outlier_percentage']:.1f}%)")
        
        # 3. æ•°æ®åˆ†å¸ƒåˆ†æ
        distribution_insights = []
        for sheet_name, df in self.sheets_data.items():
            if sheet_name == 'ğŸ“Šæ±‡æ€»ä¿¡æ¯':
                continue
                
            # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„æ•°æ®æ¨¡å¼
            if len(df) > 100:  # è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œåˆ†æ
                numeric_cols = df.select_dtypes(include=[np.number]).columns
                for col in numeric_cols[:3]:  # åªåˆ†æå‰3ä¸ªæ•°å€¼åˆ—
                    values = df[col].dropna()
                    if len(values) > 50:
                        # æ£€æŸ¥æ•°æ®åˆ†å¸ƒ
                        skewness = values.skew()
                        if abs(skewness) > 1:
                            distribution_insights.append({
                                'sheet': sheet_name,
                                'column': col,
                                'skewness': skewness,
                                'distribution': 'right_skewed' if skewness > 1 else 'left_skewed'
                            })
        
        if distribution_insights:
            print(f"\næ•°æ®åˆ†å¸ƒç‰¹å¾:")
            for insight in distribution_insights[:5]:
                dist_type = "å³å" if insight['distribution'] == 'right_skewed' else "å·¦å"
                print(f"   ğŸ“Š {insight['sheet']}.{insight['column']}: {dist_type}åˆ†å¸ƒ (ååº¦: {insight['skewness']:.2f})")
        
        insights.extend([
            {'type': 'correlations', 'data': correlations},
            {'type': 'outliers', 'data': outliers},
            {'type': 'distributions', 'data': distribution_insights}
        ])
        
        self.analysis_results['insights'] = insights
        return insights
    
    def generate_summary_report(self):
        """ç”Ÿæˆåˆ†ææ€»ç»“æŠ¥å‘Š"""
        print(f"\nğŸ“‹ ç”Ÿæˆåˆ†ææ€»ç»“æŠ¥å‘Š")
        print("-" * 50)
        
        report_file = os.path.join(ANALYSIS_OUTPUT_DIR, f"æ•°æ®åˆ†ææŠ¥å‘Š_{datetime.now().strftime('%Y%m%d_%H%M')}.txt")
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(f"å¤šSheetæ•°æ®åˆ†ææŠ¥å‘Š\n")
            f.write(f"=" * 50 + "\n")
            f.write(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"åˆ†ææ–‡ä»¶: {self.excel_file}\n\n")
            
            # æ•°æ®æ¦‚è§ˆ
            if 'overview' in self.analysis_results:
                overview = self.analysis_results['overview']
                f.write(f"æ•°æ®æ¦‚è§ˆ:\n")
                f.write(f"  æ€»Sheetæ•°: {overview['total_sheets']}\n")
                f.write(f"  æ€»è¡Œæ•°: {overview['total_rows']:,}\n\n")
                
                f.write(f"ä¸»è¦Sheetä¿¡æ¯:\n")
                for info in overview['sheet_info'][:10]:
                    f.write(f"  - {info['sheet_name']}: {info['rows']:,} è¡Œ, {info['columns']} åˆ—\n")
                f.write("\n")
            
            # æ•°æ®è´¨é‡
            if 'quality' in self.analysis_results:
                f.write(f"æ•°æ®è´¨é‡æ‘˜è¦:\n")
                quality_data = self.analysis_results['quality']
                avg_null_rate = np.mean([q['null_percentage'] for q in quality_data])
                avg_dup_rate = np.mean([q['duplicate_percentage'] for q in quality_data])
                f.write(f"  å¹³å‡ç¼ºå¤±ç‡: {avg_null_rate:.1f}%\n")
                f.write(f"  å¹³å‡é‡å¤ç‡: {avg_dup_rate:.1f}%\n\n")
            
            # æ•°å€¼åˆ†æ
            if 'numerical' in self.analysis_results:
                numerical = self.analysis_results['numerical']
                f.write(f"æ•°å€¼æ•°æ®åˆ†æ:\n")
                f.write(f"  æ€»æ•°å€¼ä¸ªæ•°: {numerical['total_values']:,}\n")
                f.write(f"  ä¸»è¦æ•°å€¼åˆ—æ•°: {len(numerical['column_summary'])}\n\n")
            
            # å…³é”®æ´å¯Ÿ
            if 'insights' in self.analysis_results:
                f.write(f"å…³é”®æ´å¯Ÿ:\n")
                insights = self.analysis_results['insights']
                for insight_group in insights:
                    if insight_group['type'] == 'correlations' and insight_group['data']:
                        f.write(f"  - å‘ç° {len(insight_group['data'])} ä¸ªé«˜ç›¸å…³æ€§åˆ—å¯¹\n")
                    elif insight_group['type'] == 'outliers' and insight_group['data']:
                        f.write(f"  - å‘ç° {len(insight_group['data'])} ä¸ªåŒ…å«å¼‚å¸¸å€¼çš„åˆ—\n")
                f.write("\n")
            
            # å»ºè®®
            f.write(f"åˆ†æå»ºè®®:\n")
            f.write(f"  1. å»ºè®®é‡ç‚¹å…³æ³¨æ•°æ®é‡æœ€å¤§çš„å‡ ä¸ªSheet\n")
            f.write(f"  2. å¯¹äºç¼ºå¤±ç‡é«˜çš„æ•°æ®ï¼Œå»ºè®®è¿›è¡Œæ•°æ®æ¸…æ´—\n")
            f.write(f"  3. å¯¹äºé«˜ç›¸å…³æ€§çš„åˆ—ï¼Œå¯ä»¥è€ƒè™‘é™ç»´æˆ–ç‰¹å¾é€‰æ‹©\n")
            f.write(f"  4. å¯¹äºåŒ…å«å¼‚å¸¸å€¼çš„åˆ—ï¼Œå»ºè®®è¿›ä¸€æ­¥è°ƒæŸ¥æ•°æ®æ¥æº\n")
        
        print(f"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        return report_file
    
    def run_full_analysis(self):
        """è¿è¡Œå®Œæ•´åˆ†ææµç¨‹"""
        print(f"ğŸš€ å¼€å§‹å¤šSheetæ•°æ®åˆ†æ")
        print(f"åˆ†ææ–‡ä»¶: {self.excel_file}")
        print("=" * 60)
        
        # 1. åŠ è½½æ•°æ®
        if not self.load_excel_data():
            return False
        
        # 2. æ•°æ®æ¦‚è§ˆ
        self.analyze_data_overview()
        
        # 3. æ•°æ®ç±»å‹åˆ†æ
        self.analyze_data_types()
        
        # 4. æ•°æ®è´¨é‡åˆ†æ
        self.analyze_data_quality()
        
        # 5. æ•°å€¼æ•°æ®åˆ†æ
        self.analyze_numerical_data()
        
        # 6. æ¨¡å¼å’Œæ´å¯Ÿåˆ†æ
        self.find_patterns_and_insights()
        
        # 7. ç”ŸæˆæŠ¥å‘Š
        report_file = self.generate_summary_report()
        
        print(f"\nğŸ‰ åˆ†æå®Œæˆ!")
        print(f"ğŸ“‹ è¯¦ç»†æŠ¥å‘Š: {report_file}")
        
        return True


def select_file_to_analyze():
    """é€‰æ‹©è¦åˆ†æçš„æ–‡ä»¶"""
    print(f"ğŸ“ æ£€æµ‹åˆ°ä»¥ä¸‹Excelæ–‡ä»¶:")
    
    available_files = []
    for i, filename in enumerate(EXCEL_FILES, 1):
        if os.path.exists(filename):
            file_size = os.path.getsize(filename) / 1024 / 1024
            available_files.append(filename)
            print(f"   {i}. {filename} ({file_size:.1f} MB)")
        else:
            print(f"   {i}. {filename} (æ–‡ä»¶ä¸å­˜åœ¨)")
    
    if not available_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯åˆ†æçš„Excelæ–‡ä»¶!")
        return None
    
    try:
        print(f"\nè¯·é€‰æ‹©è¦åˆ†æçš„æ–‡ä»¶ (1-{len(available_files)})ï¼Œæˆ–ç›´æ¥å›è½¦åˆ†æç¬¬ä¸€ä¸ªæ–‡ä»¶:")
        choice = input().strip()
        
        if not choice:
            return available_files[0]
        
        choice_idx = int(choice) - 1
        if 0 <= choice_idx < len(available_files):
            return available_files[choice_idx]
        else:
            print("æ— æ•ˆé€‰æ‹©ï¼Œå°†åˆ†æç¬¬ä¸€ä¸ªæ–‡ä»¶")
            return available_files[0]
            
    except (ValueError, KeyboardInterrupt):
        print("å°†åˆ†æç¬¬ä¸€ä¸ªæ–‡ä»¶")
        return available_files[0]


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ“Š å¤šSheetæ•°æ®åˆ†æå·¥å…·")
    print("=" * 40)
    print("ä¸“é—¨åˆ†æåˆå¹¶åçš„Excelæ–‡ä»¶ï¼Œæä¾›å…¨é¢çš„æ•°æ®æ´å¯Ÿ")
    print()
    
    # é€‰æ‹©åˆ†ææ–‡ä»¶
    excel_file = select_file_to_analyze()
    if not excel_file:
        return
    
    print(f"\nğŸ¯ å°†åˆ†ææ–‡ä»¶: {excel_file}")
    
    try:
        confirm = input("æŒ‰å›è½¦å¼€å§‹åˆ†æï¼Œè¾“å…¥nå–æ¶ˆ: ").strip().lower()
        if confirm in ['n', 'no', 'å¦']:
            print("åˆ†æå·²å–æ¶ˆ")
            return
    except KeyboardInterrupt:
        print("\nåˆ†æå·²å–æ¶ˆ")
        return
    
    # åˆ›å»ºåˆ†æå™¨å¹¶è¿è¡Œåˆ†æ
    analyzer = MultiSheetAnalyzer(excel_file)
    success = analyzer.run_full_analysis()
    
    if success:
        print(f"\nğŸ’¡ åˆ†æç»“æœå·²ä¿å­˜åˆ° '{ANALYSIS_OUTPUT_DIR}' æ–‡ä»¶å¤¹")
        print(f"   - è¯¦ç»†åˆ†ææŠ¥å‘Š")
        if HAS_PLOTTING and GENERATE_CHARTS:
            print(f"   - æ•°æ®å¯è§†åŒ–å›¾è¡¨")
    else:
        print(f"\nâŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯")


if __name__ == "__main__":
    main() 
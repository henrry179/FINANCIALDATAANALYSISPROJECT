#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤§è§„æ¨¡é‡‘èæ•°æ®å¿«é€Ÿåˆ†æè„šæœ¬
ä¸“é—¨å¤„ç†å¤§å‹åˆå¹¶æ•°æ®é›†çš„å¿«é€Ÿæ™ºèƒ½åˆ†æ
"""

import pandas as pd
import numpy as np
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# é…ç½®
DATA_FILE = "é‡‘èæ•°æ®æ±‡æ€»_20250601_1650.xlsx"  # è¾“å…¥æ•°æ®æ–‡ä»¶
SAMPLE_SIZE = 10000  # é‡‡æ ·å¤§å°ï¼Œç”¨äºå¿«é€Ÿåˆ†æ

def quick_analysis():
    """å¿«é€Ÿåˆ†æå¤§è§„æ¨¡é‡‘èæ•°æ®"""
    
    print("ğŸš€ å¯åŠ¨å¤§è§„æ¨¡é‡‘èæ•°æ®å¿«é€Ÿåˆ†æ")
    print("=" * 50)
    
    # 1. æ•°æ®åŠ è½½
    print("ğŸ“– æ­£åœ¨åŠ è½½æ•°æ®...")
    try:
        # å…ˆè¯»å–å°æ ·æœ¬æ¥äº†è§£æ•°æ®ç»“æ„
        sample_data = pd.read_excel(DATA_FILE, nrows=100)
        print(f"   âœ… æˆåŠŸåŠ è½½æ ·æœ¬æ•°æ®: {len(sample_data)} è¡Œ")
        
        # è¯»å–å®Œæ•´æ•°æ®
        print("   ğŸ“Š æ­£åœ¨åŠ è½½å®Œæ•´æ•°æ®é›†...")
        full_data = pd.read_excel(DATA_FILE)
        print(f"   âœ… æˆåŠŸåŠ è½½å®Œæ•´æ•°æ®: {len(full_data):,} è¡Œ, {len(full_data.columns)} åˆ—")
        
    except Exception as e:
        print(f"   âŒ æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
        return
    
    # 2. æ•°æ®æ¦‚è§ˆ
    print(f"\nğŸ“Š æ•°æ®é›†æ¦‚è§ˆ")
    print("-" * 40)
    print(f"æ€»è¡Œæ•°: {len(full_data):,}")
    print(f"æ€»åˆ—æ•°: {len(full_data.columns)}")
    
    # å†…å­˜ä½¿ç”¨æƒ…å†µ
    memory_usage = full_data.memory_usage(deep=True).sum() / 1024 / 1024
    print(f"å†…å­˜å ç”¨: {memory_usage:.1f} MB")
    
    # 3. è¯†åˆ«æ•°æ®ç±»å‹
    print(f"\nğŸ” æ•°æ®ç±»å‹åˆ†æ")
    print("-" * 40)
    
    numeric_cols = []
    date_cols = []
    text_cols = []
    
    for col in full_data.columns:
        col_name = str(col)
        if full_data[col].dtype in ['int64', 'float64']:
            if not col_name.startswith(('æ•°æ®æ¥æº', 'æ•°æ®æ–‡ä»¶å¤¹', 'å¤„ç†æ—¶é—´')):
                numeric_cols.append(col)
        elif 'æ—¥æœŸ' in col_name or 'æ—¶é—´' in col_name or 'date' in col_name.lower():
            date_cols.append(col)
        else:
            text_cols.append(col)
    
    print(f"æ•°å€¼åˆ—æ•°é‡: {len(numeric_cols)}")
    print(f"æ—¥æœŸåˆ—æ•°é‡: {len(date_cols)}")
    print(f"æ–‡æœ¬åˆ—æ•°é‡: {len(text_cols)}")
    
    # 4. ç¼ºå¤±å€¼åˆ†æ
    print(f"\nâš ï¸ æ•°æ®è´¨é‡åˆ†æ")
    print("-" * 40)
    
    missing_data = full_data.isnull().sum()
    missing_ratio = missing_data / len(full_data) * 100
    
    # æ‰¾å‡ºç¼ºå¤±å€¼æœ€å¤šçš„åˆ—
    high_missing = missing_ratio[missing_ratio > 50].sort_values(ascending=False)
    print(f"ç¼ºå¤±å€¼è¶…è¿‡50%çš„åˆ—æ•°: {len(high_missing)}")
    
    if len(high_missing) > 0:
        print("ç¼ºå¤±å€¼æœ€é«˜çš„å‰5åˆ—:")
        for col in high_missing.head(5).index:
            print(f"   {str(col)[:50]}...: {missing_ratio[col]:.1f}%")
    
    # 5. é‡‡æ ·åˆ†æï¼ˆé’ˆå¯¹æ•°å€¼æ•°æ®ï¼‰
    if len(numeric_cols) > 0:
        print(f"\nğŸ“ˆ æ•°å€¼æ•°æ®åˆ†æï¼ˆé‡‡æ ·åˆ†æï¼‰")
        print("-" * 40)
        
        # éšæœºé‡‡æ ·
        if len(full_data) > SAMPLE_SIZE:
            sample_data = full_data.sample(n=SAMPLE_SIZE, random_state=42)
            print(f"é‡‡ç”¨éšæœºé‡‡æ ·: {SAMPLE_SIZE:,} è¡Œ")
        else:
            sample_data = full_data
            print(f"ä½¿ç”¨å…¨é‡æ•°æ®: {len(sample_data):,} è¡Œ")
        
        # åˆ†æå‰10ä¸ªæ•°å€¼åˆ—
        numeric_sample = sample_data[numeric_cols[:10]]
        
        print(f"\nä¸»è¦æ•°å€¼åˆ—ç»Ÿè®¡:")
        for col in numeric_cols[:5]:
            if col in numeric_sample.columns:
                col_data = numeric_sample[col].dropna()
                if len(col_data) > 0:
                    print(f"   {str(col)[:30]}:")
                    print(f"      å‡å€¼: {col_data.mean():.3f}")
                    print(f"      ä¸­ä½æ•°: {col_data.median():.3f}")
                    print(f"      æ ‡å‡†å·®: {col_data.std():.3f}")
                    print(f"      èŒƒå›´: [{col_data.min():.3f}, {col_data.max():.3f}]")
    
    # 6. æ•°æ®æ¥æºåˆ†æ
    print(f"\nğŸ“ æ•°æ®æ¥æºåˆ†æ")
    print("-" * 40)
    
    if 'æ•°æ®æ¥æºæ–‡ä»¶' in full_data.columns:
        source_counts = full_data['æ•°æ®æ¥æºæ–‡ä»¶'].value_counts()
        print(f"æ•°æ®æ¥æºæ–‡ä»¶æ•°é‡: {len(source_counts)}")
        print(f"å¹³å‡æ¯æ–‡ä»¶è¡Œæ•°: {len(full_data) / len(source_counts):.1f}")
        
        print("æ•°æ®é‡æœ€å¤§çš„å‰5ä¸ªæ–‡ä»¶:")
        for file_name, count in source_counts.head(5).items():
            print(f"   {str(file_name)[:50]}...: {count:,} è¡Œ")
    
    # 7. æ—¶é—´è·¨åº¦åˆ†æ
    if len(date_cols) > 0:
        print(f"\nğŸ“… æ—¶é—´æ•°æ®åˆ†æ")
        print("-" * 40)
        
        for date_col in date_cols[:3]:  # åˆ†æå‰3ä¸ªæ—¥æœŸåˆ—
            try:
                date_data = pd.to_datetime(full_data[date_col], errors='coerce').dropna()
                if len(date_data) > 0:
                    print(f"{str(date_col)[:30]}:")
                    print(f"   æ—¶é—´è·¨åº¦: {date_data.min()} è‡³ {date_data.max()}")
                    print(f"   æœ‰æ•ˆæ—¥æœŸæ•°: {len(date_data):,}")
                    time_span = (date_data.max() - date_data.min()).days
                    print(f"   è·¨åº¦å¤©æ•°: {time_span} å¤©")
            except Exception as e:
                print(f"   {str(date_col)[:30]}: æ—¥æœŸè§£æå¤±è´¥")
    
    # 8. æ™ºèƒ½æ´å¯Ÿ
    print(f"\nğŸ’¡ æ™ºèƒ½æ´å¯Ÿ")
    print("-" * 40)
    
    insights = []
    
    # æ•°æ®è§„æ¨¡æ´å¯Ÿ
    if len(full_data) > 50000:
        insights.append("ğŸ¯ è¶…å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå»ºè®®è€ƒè™‘åˆ†æ‰¹å¤„ç†æˆ–ä½¿ç”¨åˆ†å¸ƒå¼è®¡ç®—")
    
    # åˆ—æ•°æ´å¯Ÿ
    if len(full_data.columns) > 500:
        insights.append("ğŸ“Š åˆ—æ•°é‡æå¤šï¼Œå»ºè®®è¿›è¡Œç‰¹å¾é€‰æ‹©å’Œé™ç»´åˆ†æ")
    
    # ç¼ºå¤±å€¼æ´å¯Ÿ
    overall_missing = full_data.isnull().sum().sum() / (len(full_data) * len(full_data.columns))
    if overall_missing > 0.3:
        insights.append(f"âš ï¸ æ•´ä½“ç¼ºå¤±ç‡é«˜è¾¾ {overall_missing*100:.1f}%ï¼Œå»ºè®®æ•°æ®æ¸…æ´—")
    
    # æ•°æ®ç±»å‹æ´å¯Ÿ
    if len(numeric_cols) / len(full_data.columns) > 0.8:
        insights.append("ğŸ“ˆ ä¸»è¦ä¸ºæ•°å€¼å‹æ•°æ®ï¼Œé€‚åˆè¿›è¡Œé‡åŒ–åˆ†æå’Œæœºå™¨å­¦ä¹ ")
    
    # æ•°æ®æ¥æºæ´å¯Ÿ
    if 'æ•°æ®æ¥æºæ–‡ä»¶' in full_data.columns:
        unique_sources = full_data['æ•°æ®æ¥æºæ–‡ä»¶'].nunique()
        if unique_sources > 100:
            insights.append(f"ğŸ“ æ•°æ®æ¥æºå¤šæ ·åŒ–ï¼ˆ{unique_sources}ä¸ªæ–‡ä»¶ï¼‰ï¼Œä¿¡æ¯ä¸°å¯Œåº¦é«˜")
    
    # æ—¶é—´ç»´åº¦æ´å¯Ÿ
    if len(date_cols) > 0:
        insights.append("ğŸ“… åŒ…å«æ—¶é—´ç»´åº¦ï¼Œå¯è¿›è¡Œæ—¶åºåˆ†æå’Œè¶‹åŠ¿é¢„æµ‹")
    
    if not insights:
        insights.append("âœ… æ•°æ®ç»“æ„è‰¯å¥½ï¼Œå¯è¿›è¡Œæ·±åº¦åˆ†æ")
    
    for i, insight in enumerate(insights, 1):
        print(f"   {i}. {insight}")
    
    # 9. åˆ†æå»ºè®®
    print(f"\nğŸ¯ åˆ†æå»ºè®®")
    print("-" * 40)
    recommendations = [
        "1. ä¼˜å…ˆå¤„ç†é«˜ç¼ºå¤±ç‡åˆ—ï¼Œè€ƒè™‘åˆ é™¤æˆ–æ’å€¼",
        "2. å¯¹æ•°å€¼å˜é‡è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†",
        "3. åˆ©ç”¨æ—¶é—´ç»´åº¦è¿›è¡Œè¶‹åŠ¿åˆ†æ",
        "4. è€ƒè™‘æŒ‰æ•°æ®æ¥æºè¿›è¡Œåˆ†ç»„åˆ†æ",
        "5. ä½¿ç”¨é‡‡æ ·æŠ€æœ¯è¿›è¡Œå¿«é€Ÿæ¢ç´¢æ€§åˆ†æ",
        "6. å»ºç«‹æ•°æ®å­—å…¸è®°å½•åˆ—å«ä¹‰",
        "7. è€ƒè™‘ä½¿ç”¨é™ç»´æŠ€æœ¯å¤„ç†é«˜ç»´æ•°æ®"
    ]
    
    for rec in recommendations:
        print(f"   {rec}")
    
    # 10. ç”Ÿæˆå¿«é€ŸæŠ¥å‘Š
    print(f"\nğŸ“ ç”Ÿæˆåˆ†ææŠ¥å‘Š")
    print("-" * 40)
    
    report_name = f"å¿«é€Ÿåˆ†ææŠ¥å‘Š_{datetime.now().strftime('%Y%m%d_%H%M')}.txt"
    
    report_lines = [
        "=" * 60,
        "å¤§è§„æ¨¡é‡‘èæ•°æ®å¿«é€Ÿåˆ†ææŠ¥å‘Š",
        f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        "=" * 60,
        "",
        "ğŸ“Š æ•°æ®æ¦‚è§ˆ",
        f"æ€»è¡Œæ•°: {len(full_data):,}",
        f"æ€»åˆ—æ•°: {len(full_data.columns)}",
        f"å†…å­˜å ç”¨: {memory_usage:.1f} MB",
        "",
        "ğŸ” æ•°æ®ç±»å‹åˆ†å¸ƒ",
        f"æ•°å€¼åˆ—: {len(numeric_cols)}",
        f"æ—¥æœŸåˆ—: {len(date_cols)}",
        f"æ–‡æœ¬åˆ—: {len(text_cols)}",
        "",
        "âš ï¸ æ•°æ®è´¨é‡",
        f"æ•´ä½“ç¼ºå¤±ç‡: {overall_missing*100:.1f}%",
        f"é«˜ç¼ºå¤±åˆ—æ•°: {len(high_missing)}",
        "",
        "ğŸ’¡ å…³é”®æ´å¯Ÿ",
    ]
    
    for insight in insights:
        report_lines.append(f"â€¢ {insight}")
    
    report_lines.extend([
        "",
        "ğŸ¯ åˆ†æå»ºè®®",
    ])
    
    for rec in recommendations:
        report_lines.append(f"â€¢ {rec}")
    
    report_lines.extend([
        "",
        "=" * 60,
        "æŠ¥å‘Šç»“æŸ"
    ])
    
    # ä¿å­˜æŠ¥å‘Š
    try:
        with open(report_name, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        print(f"   âœ… å¿«é€Ÿåˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_name}")
    except Exception as e:
        print(f"   âŒ æŠ¥å‘Šä¿å­˜å¤±è´¥: {str(e)}")
    
    print(f"\nğŸ‰ å¿«é€Ÿåˆ†æå®Œæˆï¼")
    print(f"ğŸ“Š æ•°æ®æ–‡ä»¶: {DATA_FILE}")
    print(f"ğŸ“ åˆ†ææŠ¥å‘Š: {report_name}")

if __name__ == "__main__":
    quick_analysis() 
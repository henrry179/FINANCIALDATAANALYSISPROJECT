#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é‡‘èæ•°æ®åˆå¹¶ä¸æ™ºèƒ½åˆ†æè„šæœ¬
ä¸“é—¨ç”¨äºåˆå¹¶é‡‘èç›¸å…³çš„Excelå’ŒCSVæ•°æ®è¡¨ï¼Œå¹¶æä¾›å¤šç»´åº¦æ™ºèƒ½åˆ†æ
"""

import os
import pandas as pd
import numpy as np
import glob
from pathlib import Path
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# å°è¯•å¯¼å…¥å¯è§†åŒ–å’Œç»Ÿè®¡åˆ†æåº“
try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    HAS_PLOTTING = True
except ImportError:
    print("ğŸ“Š æç¤º: å®‰è£… matplotlib å’Œ seaborn å¯è·å¾—å¯è§†åŒ–åŠŸèƒ½")
    HAS_PLOTTING = False

try:
    from scipy import stats
    from sklearn.preprocessing import StandardScaler
    from sklearn.decomposition import PCA
    HAS_ADVANCED_STATS = True
except ImportError:
    print("ğŸ“ˆ æç¤º: å®‰è£… scipy å’Œ scikit-learn å¯è·å¾—é«˜çº§ç»Ÿè®¡åˆ†æåŠŸèƒ½")
    HAS_ADVANCED_STATS = False


# ====== é‡‘èæ•°æ®é…ç½®åŒºåŸŸ ======

# é‡‘èæ•°æ®æ–‡ä»¶å¤¹è·¯å¾„
FINANCE_DATA_FOLDERS = [
    "/Users/mac/Downloads/WorkFiles/financedatasets01-0601",  # é‡‘èæ•°æ®é›†
    "/Users/mac/Downloads/WorkFiles/financedatasets02-0601",  # é‡‘èæ•°æ®é›†
    "/Users/mac/Downloads/WorkFiles/financedatasets03-0601",  # é‡‘èæ•°æ®é›†
    "/Users/mac/Downloads/WorkFiles/financedatasets04-0601",  # é‡‘èæ•°æ®é›†

    # å¯ä»¥æ·»åŠ æ›´å¤šé‡‘èæ•°æ®æ–‡ä»¶å¤¹è·¯å¾„
    # "/Users/mac/Documents/é‡‘èæ•°æ®2024",
    # "/Users/mac/Downloads/è‚¡ç¥¨æ•°æ®",
]

# è¾“å‡ºæ–‡ä»¶é…ç½®
OUTPUT_FILE = f"é‡‘èæ•°æ®æ±‡æ€»_{datetime.now().strftime('%Y%m%d_%H%M')}.xlsx"
ANALYSIS_REPORT = f"é‡‘èæ•°æ®åˆ†ææŠ¥å‘Š_{datetime.now().strftime('%Y%m%d_%H%M')}.txt"

# æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
SUPPORTED_FORMATS = ['.xlsx', '.csv', '.xls']

# åŠŸèƒ½å¼€å…³
AUTO_RUN = True
ENABLE_ANALYSIS = True  # æ˜¯å¦å¯ç”¨æ•°æ®åˆ†æåŠŸèƒ½
SAVE_PLOTS = True       # æ˜¯å¦ä¿å­˜å›¾è¡¨
DETAILED_REPORT = True  # æ˜¯å¦ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š

# ====== é…ç½®åŒºåŸŸç»“æŸ ======


class FinanceDataAnalyzer:
    """é‡‘èæ•°æ®æ™ºèƒ½åˆ†æå™¨"""
    
    def __init__(self, data):
        self.data = data
        self.numeric_cols = []
        self.date_cols = []
        self.analysis_results = {}
        self._prepare_data()
    
    def _prepare_data(self):
        """æ•°æ®é¢„å¤„ç†å’Œç±»å‹è¯†åˆ«"""
        print("ğŸ” æ­£åœ¨åˆ†ææ•°æ®ç»“æ„...")
        
        # è¯†åˆ«æ•°å€¼åˆ—
        for col in self.data.columns:
            # ç¡®ä¿åˆ—åæ˜¯å­—ç¬¦ä¸²ç±»å‹
            col_name = str(col)
            if self.data[col].dtype in ['int64', 'float64']:
                if not col_name.startswith(('æ•°æ®æ¥æº', 'æ•°æ®æ–‡ä»¶å¤¹', 'å¤„ç†æ—¶é—´')):
                    self.numeric_cols.append(col)
        
        # è¯†åˆ«æ—¥æœŸåˆ—
        for col in self.data.columns:
            col_name = str(col)
            if 'æ—¥æœŸ' in col_name or 'æ—¶é—´' in col_name or 'date' in col_name.lower():
                try:
                    pd.to_datetime(self.data[col])
                    self.date_cols.append(col)
                except:
                    pass
        
        print(f"   ğŸ“Š è¯†åˆ«åˆ° {len(self.numeric_cols)} ä¸ªæ•°å€¼åˆ—")
        print(f"   ğŸ“… è¯†åˆ«åˆ° {len(self.date_cols)} ä¸ªæ—¥æœŸåˆ—")
    
    def basic_statistics(self):
        """åŸºç¡€ç»Ÿè®¡åˆ†æ"""
        print("\nğŸ“ˆ 1. åŸºç¡€ç»Ÿè®¡åˆ†æ")
        print("-" * 40)
        
        if not self.numeric_cols:
            print("   âš ï¸  æœªå‘ç°æ•°å€¼å‹æ•°æ®ï¼Œè·³è¿‡ç»Ÿè®¡åˆ†æ")
            return {}
        
        # æè¿°æ€§ç»Ÿè®¡
        numeric_data = self.data[self.numeric_cols]
        desc_stats = numeric_data.describe()
        
        # è®¡ç®—é¢å¤–ç»Ÿè®¡æŒ‡æ ‡
        additional_stats = {}
        for col in self.numeric_cols[:10]:  # é™åˆ¶åˆ†æå‰10åˆ—ï¼Œé¿å…è¾“å‡ºè¿‡é•¿
            col_data = numeric_data[col].dropna()
            if len(col_data) > 0:
                additional_stats[col] = {
                    'ç¼ºå¤±å€¼æ•°é‡': self.data[col].isnull().sum(),
                    'ç¼ºå¤±å€¼æ¯”ä¾‹': f"{self.data[col].isnull().mean()*100:.2f}%",
                    'å”¯ä¸€å€¼æ•°é‡': self.data[col].nunique(),
                    'ååº¦': f"{col_data.skew():.3f}",
                    'å³°åº¦': f"{col_data.kurtosis():.3f}",
                }
        
        # æ˜¾ç¤ºå…³é”®ç»Ÿè®¡ä¿¡æ¯
        print("   ğŸ“Š æ•°æ®æ¦‚è§ˆ:")
        print(f"      æ€»è¡Œæ•°: {len(self.data):,}")
        print(f"      æ€»åˆ—æ•°: {len(self.data.columns)}")
        print(f"      æ•°å€¼åˆ—æ•°: {len(self.numeric_cols)}")
        
        print("\n   ğŸ“‹ ä¸»è¦æ•°å€¼åˆ—ç»Ÿè®¡ï¼ˆå‰5åˆ—ï¼‰:")
        for col in self.numeric_cols[:5]:
            if col in additional_stats:
                stats = additional_stats[col]
                print(f"      {col}:")
                print(f"         å‡å€¼: {numeric_data[col].mean():.3f}")
                print(f"         æ ‡å‡†å·®: {numeric_data[col].std():.3f}")
                print(f"         ç¼ºå¤±å€¼: {stats['ç¼ºå¤±å€¼æ•°é‡']} ({stats['ç¼ºå¤±å€¼æ¯”ä¾‹']})")
        
        self.analysis_results['basic_stats'] = {
            'desc_stats': desc_stats,
            'additional_stats': additional_stats
        }
        
        return self.analysis_results['basic_stats']
    
    def trend_analysis(self):
        """è¶‹åŠ¿åˆ†æ"""
        print("\nğŸ“ˆ 2. è¶‹åŠ¿åˆ†æ")
        print("-" * 40)
        
        if not self.date_cols or not self.numeric_cols:
            print("   âš ï¸  ç¼ºå°‘æ—¥æœŸæˆ–æ•°å€¼æ•°æ®ï¼Œè·³è¿‡è¶‹åŠ¿åˆ†æ")
            return {}
        
        trends = {}
        
        # åˆ†ææ—¶é—´åºåˆ—è¶‹åŠ¿
        for date_col in self.date_cols[:2]:  # é™åˆ¶åˆ†æå‰2ä¸ªæ—¥æœŸåˆ—
            try:
                # è½¬æ¢æ—¥æœŸåˆ—
                self.data[date_col] = pd.to_datetime(self.data[date_col])
                
                # æŒ‰æ—¥æœŸæ’åº
                temp_data = self.data.sort_values(date_col)
                
                print(f"   ğŸ“… åˆ†ææ—¶é—´åºåˆ—: {date_col}")
                print(f"      æ—¶é—´èŒƒå›´: {temp_data[date_col].min()} è‡³ {temp_data[date_col].max()}")
                
                # åˆ†ææ•°å€¼åˆ—çš„è¶‹åŠ¿
                for num_col in self.numeric_cols[:3]:  # é™åˆ¶åˆ†æå‰3ä¸ªæ•°å€¼åˆ—
                    if num_col in temp_data.columns:
                        valid_data = temp_data[[date_col, num_col]].dropna()
                        if len(valid_data) > 10:
                            # è®¡ç®—è¶‹åŠ¿
                            x = np.arange(len(valid_data))
                            y = valid_data[num_col].values
                            
                            if HAS_ADVANCED_STATS:
                                slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)
                                trend_direction = "ä¸Šå‡" if slope > 0 else "ä¸‹é™"
                                trend_strength = abs(r_value)
                                
                                trends[f"{date_col}_{num_col}"] = {
                                    'slope': slope,
                                    'r_squared': r_value**2,
                                    'direction': trend_direction,
                                    'strength': trend_strength,
                                    'significance': 'significant' if p_value < 0.05 else 'not_significant'
                                }
                                
                                print(f"      {num_col}: {trend_direction}è¶‹åŠ¿ (RÂ²={r_value**2:.3f})")
            except Exception as e:
                print(f"   âš ï¸  {date_col} è¶‹åŠ¿åˆ†æå¤±è´¥: {str(e)}")
        
        self.analysis_results['trends'] = trends
        return trends
    
    def correlation_analysis(self):
        """ç›¸å…³æ€§åˆ†æ"""
        print("\nğŸ”— 3. ç›¸å…³æ€§åˆ†æ")
        print("-" * 40)
        
        if len(self.numeric_cols) < 2:
            print("   âš ï¸  æ•°å€¼åˆ—å°‘äº2ä¸ªï¼Œè·³è¿‡ç›¸å…³æ€§åˆ†æ")
            return {}
        
        # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
        numeric_data = self.data[self.numeric_cols[:10]].dropna()  # é™åˆ¶å‰10åˆ—ï¼Œåˆ é™¤ç¼ºå¤±å€¼
        
        if len(numeric_data) < 10:
            print("   âš ï¸  æœ‰æ•ˆæ•°æ®ä¸è¶³ï¼Œè·³è¿‡ç›¸å…³æ€§åˆ†æ")
            return {}
        
        corr_matrix = numeric_data.corr()
        
        # æ‰¾å‡ºå¼ºç›¸å…³å…³ç³»
        strong_correlations = []
        for i in range(len(corr_matrix.columns)):
            for j in range(i+1, len(corr_matrix.columns)):
                corr_value = corr_matrix.iloc[i, j]
                if abs(corr_value) > 0.7:  # å¼ºç›¸å…³é˜ˆå€¼
                    strong_correlations.append({
                        'var1': corr_matrix.columns[i],
                        'var2': corr_matrix.columns[j],
                        'correlation': corr_value,
                        'strength': 'å¼ºæ­£ç›¸å…³' if corr_value > 0.7 else 'å¼ºè´Ÿç›¸å…³'
                    })
        
        print(f"   ğŸ” å‘ç° {len(strong_correlations)} å¯¹å¼ºç›¸å…³å˜é‡:")
        for corr in strong_correlations[:5]:  # æ˜¾ç¤ºå‰5å¯¹
            print(f"      {corr['var1']} â†” {corr['var2']}: {corr['correlation']:.3f} ({corr['strength']})")
        
        self.analysis_results['correlations'] = {
            'matrix': corr_matrix,
            'strong_correlations': strong_correlations
        }
        
        # ä¿å­˜ç›¸å…³æ€§çƒ­å›¾
        if HAS_PLOTTING and SAVE_PLOTS:
            try:
                plt.figure(figsize=(12, 10))
                sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, 
                           fmt='.2f', square=True)
                plt.title('é‡‘èæ•°æ®ç›¸å…³æ€§çƒ­å›¾')
                plt.tight_layout()
                plt.savefig('é‡‘èæ•°æ®ç›¸å…³æ€§çƒ­å›¾.png', dpi=300, bbox_inches='tight')
                plt.close()
                print("   ğŸ“Š ç›¸å…³æ€§çƒ­å›¾å·²ä¿å­˜ä¸º 'é‡‘èæ•°æ®ç›¸å…³æ€§çƒ­å›¾.png'")
            except Exception as e:
                print(f"   âš ï¸  ä¿å­˜çƒ­å›¾å¤±è´¥: {str(e)}")
        
        return self.analysis_results['correlations']
    
    def risk_analysis(self):
        """é£é™©åˆ†æ"""
        print("\nâš ï¸  4. é£é™©åˆ†æ")
        print("-" * 40)
        
        if not self.numeric_cols:
            print("   âš ï¸  æ— æ•°å€¼æ•°æ®ï¼Œè·³è¿‡é£é™©åˆ†æ")
            return {}
        
        risk_metrics = {}
        
        # è®¡ç®—å„ç±»é£é™©æŒ‡æ ‡
        for col in self.numeric_cols[:5]:  # é™åˆ¶å‰5åˆ—
            col_data = self.data[col].dropna()
            if len(col_data) > 10:
                # æ³¢åŠ¨æ€§åˆ†æ
                volatility = col_data.std() / abs(col_data.mean()) if col_data.mean() != 0 else float('inf')
                
                # VaRè®¡ç®— (Value at Risk)
                var_95 = np.percentile(col_data, 5)
                var_99 = np.percentile(col_data, 1)
                
                # æå€¼åˆ†æ
                q1 = col_data.quantile(0.25)
                q3 = col_data.quantile(0.75)
                iqr = q3 - q1
                outliers = col_data[(col_data < q1 - 1.5*iqr) | (col_data > q3 + 1.5*iqr)]
                
                risk_metrics[col] = {
                    'volatility': volatility,
                    'var_95': var_95,
                    'var_99': var_99,
                    'outliers_count': len(outliers),
                    'outliers_ratio': len(outliers) / len(col_data) * 100,
                    'max_drawdown': (col_data.max() - col_data.min()) / col_data.max() if col_data.max() != 0 else 0
                }
                
                risk_level = "é«˜" if volatility > 0.5 else "ä¸­" if volatility > 0.2 else "ä½"
                print(f"   ğŸ“Š {col}:")
                print(f"      æ³¢åŠ¨æ€§: {volatility:.3f} (é£é™©ç­‰çº§: {risk_level})")
                print(f"      VaR(95%): {var_95:.3f}")
                print(f"      å¼‚å¸¸å€¼æ¯”ä¾‹: {len(outliers) / len(col_data) * 100:.2f}%")
        
        self.analysis_results['risk_analysis'] = risk_metrics
        return risk_metrics
    
    def market_insights(self):
        """å¸‚åœºæ´å¯Ÿåˆ†æ"""
        print("\nğŸ’¡ 5. å¸‚åœºæ´å¯Ÿåˆ†æ")
        print("-" * 40)
        
        insights = []
        
        # æ•°æ®å®Œæ•´æ€§åˆ†æ
        missing_ratio = self.data.isnull().sum().sum() / (len(self.data) * len(self.data.columns))
        if missing_ratio > 0.1:
            insights.append(f"âš ï¸  æ•°æ®ç¼ºå¤±ç‡è¾ƒé«˜ ({missing_ratio*100:.1f}%)ï¼Œå»ºè®®å…³æ³¨æ•°æ®è´¨é‡")
        
        # æ•°å€¼åˆ†å¸ƒåˆ†æ
        if self.numeric_cols:
            for col in self.numeric_cols[:3]:
                col_data = self.data[col].dropna()
                if len(col_data) > 100:
                    skewness = col_data.skew()
                    if abs(skewness) > 1:
                        skew_type = "å³å" if skewness > 0 else "å·¦å"
                        insights.append(f"ğŸ“ˆ {col} å‘ˆç°{skew_type}åˆ†å¸ƒï¼Œå¯èƒ½å­˜åœ¨æç«¯å€¼å½±å“")
        
        # æ—¶é—´åºåˆ—åˆ†æ
        if self.date_cols and 'trends' in self.analysis_results:
            trend_count = len(self.analysis_results['trends'])
            if trend_count > 0:
                insights.append(f"ğŸ“… æ£€æµ‹åˆ° {trend_count} ä¸ªæ—¶é—´åºåˆ—è¶‹åŠ¿ï¼Œå»ºè®®å…³æ³¨æ—¶é—´æ•ˆåº”")
        
        # ç›¸å…³æ€§æ´å¯Ÿ
        if 'correlations' in self.analysis_results:
            strong_corr_count = len(self.analysis_results['correlations']['strong_correlations'])
            if strong_corr_count > 5:
                insights.append(f"ğŸ”— å‘ç° {strong_corr_count} å¯¹å¼ºç›¸å…³å˜é‡ï¼Œå­˜åœ¨å¤šé‡å…±çº¿æ€§é£é™©")
        
        # é£é™©è¯„ä¼°
        if 'risk_analysis' in self.analysis_results:
            high_risk_vars = sum(1 for metrics in self.analysis_results['risk_analysis'].values() 
                               if metrics['volatility'] > 0.5)
            if high_risk_vars > 0:
                insights.append(f"âš ï¸  å‘ç° {high_risk_vars} ä¸ªé«˜é£é™©å˜é‡ï¼Œå»ºè®®åŠ å¼ºé£é™©ç®¡æ§")
        
        # æ•°æ®è§„æ¨¡æ´å¯Ÿ
        data_size_mb = self.data.memory_usage(deep=True).sum() / 1024 / 1024
        if data_size_mb > 100:
            insights.append(f"ğŸ’¾ æ•°æ®é‡è¾ƒå¤§ ({data_size_mb:.1f}MB)ï¼Œå»ºè®®è€ƒè™‘åˆ†æ‰¹å¤„ç†æˆ–ä¼˜åŒ–å­˜å‚¨")
        
        print("   ğŸ” å…³é”®æ´å¯Ÿ:")
        for i, insight in enumerate(insights, 1):
            print(f"      {i}. {insight}")
        
        if not insights:
            insights.append("âœ… æ•°æ®è´¨é‡è‰¯å¥½ï¼Œæœªå‘ç°æ˜æ˜¾å¼‚å¸¸")
            print("   âœ… æ•°æ®è´¨é‡è‰¯å¥½ï¼Œæœªå‘ç°æ˜æ˜¾å¼‚å¸¸")
        
        self.analysis_results['insights'] = insights
        return insights
    
    def generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        print("\nğŸ“ 6. ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š")
        print("-" * 40)
        
        report_lines = [
            "=" * 80,
            f"é‡‘èæ•°æ®æ™ºèƒ½åˆ†ææŠ¥å‘Š",
            f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "=" * 80,
            "",
            "ğŸ“Š æ•°æ®æ¦‚è§ˆ",
            "-" * 40,
            f"æ•°æ®è¡Œæ•°: {len(self.data):,}",
            f"æ•°æ®åˆ—æ•°: {len(self.data.columns)}",
            f"æ•°å€¼åˆ—æ•°: {len(self.numeric_cols)}",
            f"æ—¥æœŸåˆ—æ•°: {len(self.date_cols)}",
            "",
        ]
        
        # æ·»åŠ åŸºç¡€ç»Ÿè®¡
        if 'basic_stats' in self.analysis_results:
            report_lines.extend([
                "ğŸ“ˆ åŸºç¡€ç»Ÿè®¡æ‘˜è¦",
                "-" * 40,
            ])
            for col in self.numeric_cols[:5]:
                if col in self.data.columns:
                    mean_val = self.data[col].mean()
                    std_val = self.data[col].std()
                    report_lines.append(f"{col}: å‡å€¼={mean_val:.3f}, æ ‡å‡†å·®={std_val:.3f}")
            report_lines.append("")
        
        # æ·»åŠ ç›¸å…³æ€§åˆ†æ
        if 'correlations' in self.analysis_results:
            strong_corrs = self.analysis_results['correlations']['strong_correlations']
            report_lines.extend([
                "ğŸ”— å¼ºç›¸å…³å…³ç³»",
                "-" * 40,
            ])
            for corr in strong_corrs[:10]:
                report_lines.append(f"{corr['var1']} â†” {corr['var2']}: {corr['correlation']:.3f}")
            report_lines.append("")
        
        # æ·»åŠ é£é™©åˆ†æ
        if 'risk_analysis' in self.analysis_results:
            report_lines.extend([
                "âš ï¸ é£é™©è¯„ä¼°",
                "-" * 40,
            ])
            for var, metrics in list(self.analysis_results['risk_analysis'].items())[:5]:
                risk_level = "é«˜" if metrics['volatility'] > 0.5 else "ä¸­" if metrics['volatility'] > 0.2 else "ä½"
                report_lines.append(f"{var}: æ³¢åŠ¨æ€§={metrics['volatility']:.3f} (é£é™©ç­‰çº§: {risk_level})")
            report_lines.append("")
        
        # æ·»åŠ å¸‚åœºæ´å¯Ÿ
        if 'insights' in self.analysis_results:
            report_lines.extend([
                "ğŸ’¡ å…³é”®æ´å¯Ÿ",
                "-" * 40,
            ])
            for insight in self.analysis_results['insights']:
                report_lines.append(f"â€¢ {insight}")
            report_lines.append("")
        
        # æ·»åŠ å»ºè®®
        report_lines.extend([
            "ğŸ¯ åˆ†æå»ºè®®",
            "-" * 40,
            "1. å®šæœŸç›‘æ§é«˜é£é™©å˜é‡çš„æ³¢åŠ¨æƒ…å†µ",
            "2. å…³æ³¨å¼ºç›¸å…³å˜é‡é—´çš„å…³ç³»å˜åŒ–",
            "3. å»ºç«‹é¢„è­¦æœºåˆ¶è¯†åˆ«å¼‚å¸¸æ•°æ®",
            "4. è€ƒè™‘ä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹è¿›è¡Œé¢„æµ‹åˆ†æ",
            "5. æŒç»­æ”¶é›†å’Œæ›´æ–°æ•°æ®ä»¥æé«˜åˆ†æå‡†ç¡®æ€§",
            "",
            "=" * 80,
            "æŠ¥å‘Šç»“æŸ"
        ])
        
        # ä¿å­˜æŠ¥å‘Š
        try:
            with open(ANALYSIS_REPORT, 'w', encoding='utf-8') as f:
                f.write('\n'.join(report_lines))
            print(f"   âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜ä¸º: {ANALYSIS_REPORT}")
        except Exception as e:
            print(f"   âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {str(e)}")
        
        return '\n'.join(report_lines)
    
    def run_full_analysis(self):
        """è¿è¡Œå®Œæ•´åˆ†ææµç¨‹"""
        print("\nğŸ¤– å¯åŠ¨é‡‘èæ•°æ®æ™ºèƒ½åˆ†æç³»ç»Ÿ")
        print("=" * 50)
        
        # æ‰§è¡Œå„é¡¹åˆ†æ
        self.basic_statistics()
        self.trend_analysis()
        self.correlation_analysis()
        self.risk_analysis()
        self.market_insights()
        
        if DETAILED_REPORT:
            self.generate_comprehensive_report()
        
        print("\nâœ… æ™ºèƒ½åˆ†æå®Œæˆ!")
        return self.analysis_results


def merge_finance_data():
    """åˆå¹¶é‡‘èæ•°æ®æ–‡ä»¶"""
    
    print("ğŸ¦ é‡‘èæ•°æ®åˆå¹¶å·¥å…·")
    print("=" * 40)
    
    # è®°å½•å¤„ç†ä¿¡æ¯
    all_files = []
    processed_files = 0
    error_files = []
    
    # 1. æ‰«ææ‰€æœ‰æŒ‡å®šæ–‡ä»¶å¤¹
    print(f"\næ­£åœ¨æ‰«æ {len(FINANCE_DATA_FOLDERS)} ä¸ªé‡‘èæ•°æ®æ–‡ä»¶å¤¹...")
    
    for folder_path in FINANCE_DATA_FOLDERS:
        if not os.path.exists(folder_path):
            print(f"âš ï¸  è­¦å‘Š: æ–‡ä»¶å¤¹ '{folder_path}' ä¸å­˜åœ¨ï¼Œè·³è¿‡...")
            continue
        
        print(f"ğŸ“ æ‰«ææ–‡ä»¶å¤¹: {folder_path}")
        
        # é€’å½’æŸ¥æ‰¾æ‰€æœ‰æ”¯æŒæ ¼å¼çš„æ–‡ä»¶
        for ext in SUPPORTED_FORMATS:
            pattern = os.path.join(folder_path, '**', f'*{ext}')
            files = glob.glob(pattern, recursive=True)
            all_files.extend(files)
            print(f"   ğŸ“Š æ‰¾åˆ° {len(files)} ä¸ª {ext} æ–‡ä»¶")
    
    print(f"\nğŸ“ˆ æ€»å…±æ‰¾åˆ° {len(all_files)} ä¸ªé‡‘èæ•°æ®æ–‡ä»¶")
    
    if not all_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ”¯æŒçš„é‡‘èæ•°æ®æ–‡ä»¶!")
        return None
    
    # 2. æŒ‰æ–‡ä»¶ç±»å‹åˆ†ç±»æ˜¾ç¤º
    print("\nğŸ“‹ æ–‡ä»¶ç±»å‹åˆ†æ:")
    file_types = {}
    for file_path in all_files:
        filename = os.path.basename(file_path).lower()
        
        # ç®€å•åˆ†ç±»
        if any(keyword in filename for keyword in ['è‚¡ç¥¨', 'è‚¡æŒ‡', 'aè‚¡', 'ä¸Šè¯', 'æ·±è¯']):
            category = "ğŸ“ˆ è‚¡ç¥¨æ•°æ®"
        elif any(keyword in filename for keyword in ['æ±‡ç‡', 'å¤–æ±‡', 'äººæ°‘å¸']):
            category = "ğŸ’± æ±‡ç‡æ•°æ®"  
        elif any(keyword in filename for keyword in ['åˆ©ç‡', 'shibor', 'lpr', 'æ‹†å€Ÿ']):
            category = "ğŸ’° åˆ©ç‡æ•°æ®"
        elif any(keyword in filename for keyword in ['é“¶è¡Œ', 'å·¥å•†', 'å»ºè®¾', 'äº¤é€š']):
            category = "ğŸ¦ é“¶è¡Œæ•°æ®"
        elif any(keyword in filename for keyword in ['è´§å¸', 'm2', 'ä¾›åº”é‡']):
            category = "ğŸ’¸ è´§å¸æ•°æ®"
        else:
            category = "ğŸ“Š å…¶ä»–æ•°æ®"
            
        if category not in file_types:
            file_types[category] = 0
        file_types[category] += 1
    
    for category, count in file_types.items():
        print(f"   {category}: {count} ä¸ªæ–‡ä»¶")
    
    # 3. è¯»å–å¹¶åˆå¹¶æ‰€æœ‰æ–‡ä»¶
    print(f"\nğŸ”„ å¼€å§‹è¯»å–å’Œåˆå¹¶æ•°æ®...")
    all_dataframes = []
    
    for i, file_path in enumerate(all_files, 1):
        try:
            filename = os.path.basename(file_path)
            print(f"ğŸ“– [{i}/{len(all_files)}] æ­£åœ¨å¤„ç†: {filename}")
            
            # æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©è¯»å–æ–¹æ³•
            file_ext = Path(file_path).suffix.lower()
            
            if file_ext in ['.xlsx', '.xls']:
                df = pd.read_excel(file_path)
            elif file_ext == '.csv':
                # å°è¯•ä¸åŒç¼–ç è¯»å–CSVæ–‡ä»¶
                try:
                    df = pd.read_csv(file_path, encoding='utf-8')
                except UnicodeDecodeError:
                    try:
                        df = pd.read_csv(file_path, encoding='gbk')
                    except UnicodeDecodeError:
                        df = pd.read_csv(file_path, encoding='latin-1')
            
            # æ·»åŠ æ–‡ä»¶æ¥æºä¿¡æ¯
            df['æ•°æ®æ¥æºæ–‡ä»¶'] = filename
            df['æ•°æ®æ–‡ä»¶å¤¹'] = os.path.dirname(file_path)
            df['å¤„ç†æ—¶é—´'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            
            all_dataframes.append(df)
            processed_files += 1
            print(f"   âœ… æˆåŠŸè¯»å– {len(df)} è¡Œæ•°æ®ï¼Œ{len(df.columns)} åˆ—")
            
        except Exception as e:
            error_msg = f"è¯»å–å¤±è´¥: {str(e)}"
            print(f"   âŒ {error_msg}")
            error_files.append((file_path, error_msg))
    
    # 4. åˆå¹¶æ‰€æœ‰æ•°æ®
    if all_dataframes:
        print(f"\nğŸ”— æ­£åœ¨åˆå¹¶ {len(all_dataframes)} ä¸ªæ–‡ä»¶çš„æ•°æ®...")
        merged_data = pd.concat(all_dataframes, ignore_index=True, sort=False)
        
        # 5. ä¿å­˜åˆå¹¶åçš„æ•°æ®
        print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜æ•°æ®åˆ°: {OUTPUT_FILE}")
        
        try:
            output_ext = Path(OUTPUT_FILE).suffix.lower()
            
            if output_ext == '.xlsx':
                merged_data.to_excel(OUTPUT_FILE, index=False)
            elif output_ext == '.csv':
                merged_data.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')
            else:
                # é»˜è®¤ä¿å­˜ä¸ºExcel
                output_file_with_ext = OUTPUT_FILE + '.xlsx'
                merged_data.to_excel(output_file_with_ext, index=False)
                print(f"å·²è‡ªåŠ¨æ·»åŠ .xlsxæ‰©å±•å: {output_file_with_ext}")
            
            print("\nğŸ‰ é‡‘èæ•°æ®åˆå¹¶å®Œæˆ!")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")
            return None
    
    else:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®å¯ä»¥åˆå¹¶!")
        return None
    
    # 6. æ˜¾ç¤ºè¯¦ç»†å¤„ç†æ‘˜è¦
    print("\n" + "=" * 60)
    print("ğŸ“Š é‡‘èæ•°æ®å¤„ç†æ‘˜è¦")
    print("=" * 60)
    print(f"âœ… æˆåŠŸå¤„ç†æ–‡ä»¶æ•°: {processed_files}")
    print(f"ğŸ“ˆ åˆå¹¶åæ€»è¡Œæ•°: {len(merged_data):,}")
    print(f"ğŸ“‹ åˆå¹¶åæ€»åˆ—æ•°: {len(merged_data.columns)}")
    print(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶: {OUTPUT_FILE}")
    print(f"ğŸ“‚ æ–‡ä»¶å¤§å°: {os.path.getsize(OUTPUT_FILE) / 1024:.1f} KB")
    
    if error_files:
        print(f"\nâŒ å¤„ç†å¤±è´¥æ–‡ä»¶æ•°: {len(error_files)}")
        for file_path, error in error_files:
            print(f"   - {os.path.basename(file_path)}: {error}")
    
    print(f"\nğŸ“‹ æ•°æ®æ¦‚è§ˆï¼ˆå‰3è¡Œï¼‰:")
    pd.set_option('display.max_columns', 10)
    pd.set_option('display.width', 1000)
    print(merged_data.head(3))
    
    print(f"\nğŸ’¡ æç¤º: æ‚¨å¯ä»¥ä½¿ç”¨Excelæˆ–å…¶ä»–æ•°æ®åˆ†æå·¥å…·æ‰“å¼€ '{OUTPUT_FILE}' æŸ¥çœ‹å®Œæ•´æ•°æ®")
    
    return merged_data


if __name__ == "__main__":
    # è¿è¡Œå‰æ£€æŸ¥é…ç½®
    print("å½“å‰é…ç½®:")
    print(f"ğŸ“ é‡‘èæ•°æ®æ–‡ä»¶å¤¹: {FINANCE_DATA_FOLDERS}")
    print(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶: {OUTPUT_FILE}")
    print(f"ğŸ“Š æ”¯æŒæ ¼å¼: {SUPPORTED_FORMATS}")
    print(f"ğŸ”¬ æ™ºèƒ½åˆ†æ: {'å¯ç”¨' if ENABLE_ANALYSIS else 'ç¦ç”¨'}")
    
    # ç­‰å¾…ç”¨æˆ·ç¡®è®¤æˆ–è‡ªåŠ¨è¿è¡Œ
    if AUTO_RUN:
        print("\nğŸš€ è‡ªåŠ¨è¿è¡Œæ¨¡å¼å·²å¯ç”¨ï¼Œå¼€å§‹åˆå¹¶...")
        
        # 1. æ•°æ®åˆå¹¶
        merged_data = merge_finance_data()
        
        # 2. æ™ºèƒ½åˆ†æ
        if merged_data is not None and ENABLE_ANALYSIS:
            analyzer = FinanceDataAnalyzer(merged_data)
            analysis_results = analyzer.run_full_analysis()
            
            print(f"\nğŸ“‹ åˆ†æå®Œæˆï¼ç”Ÿæˆäº†ä»¥ä¸‹æ–‡ä»¶:")
            print(f"   ğŸ“Š æ•°æ®æ–‡ä»¶: {OUTPUT_FILE}")
            if DETAILED_REPORT:
                print(f"   ğŸ“ åˆ†ææŠ¥å‘Š: {ANALYSIS_REPORT}")
            if HAS_PLOTTING and SAVE_PLOTS:
                print(f"   ğŸ“ˆ å¯è§†åŒ–å›¾è¡¨: é‡‘èæ•°æ®ç›¸å…³æ€§çƒ­å›¾.png")
        
    else:
        confirm = input(f"\næ˜¯å¦ä½¿ç”¨å½“å‰é…ç½®å¼€å§‹åˆå¹¶é‡‘èæ•°æ®? (y/n): ").lower().strip()
        
        if confirm == 'y':
            merged_data = merge_finance_data()
            
            if merged_data is not None and ENABLE_ANALYSIS:
                run_analysis = input("æ˜¯å¦è¿è¡Œæ™ºèƒ½åˆ†æ? (y/n): ").lower().strip() == 'y'
                if run_analysis:
                    analyzer = FinanceDataAnalyzer(merged_data)
                    analyzer.run_full_analysis()
        else:
            print("è¯·ä¿®æ”¹è„šæœ¬é¡¶éƒ¨çš„é…ç½®åé‡æ–°è¿è¡Œ")
            print("ä¸»è¦éœ€è¦ä¿®æ”¹:")
            print("1. FINANCE_DATA_FOLDERS - æ‚¨çš„é‡‘èæ•°æ®æ–‡ä»¶å¤¹è·¯å¾„")
            print("2. OUTPUT_FILE - è¾“å‡ºæ–‡ä»¶åå’Œè·¯å¾„")
            print("3. ENABLE_ANALYSIS - æ˜¯å¦å¯ç”¨æ™ºèƒ½åˆ†æåŠŸèƒ½") 